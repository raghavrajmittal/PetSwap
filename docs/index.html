<!DOCTYPE html>
<html lang="en"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Class Project
  | Georgia Tech | Fall 2018: CS 6476 </title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">


<!-- Le styles -->
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>

<link href="css/bootstrap-responsive.min.css" rel="stylesheet">
</head>

<body>
<div class="container">
<div class="page-header">



<!-- Title and Name -->
<h1>PetSwap</h1>
<span style="font-size: 20px; line-height: 1.5em;"><strong>Hemanth Chittanuru (hchittanuru3),
  Kenny Scharm (kscharm3), Raghav Raj Mittal (rmittal34), Sarah Li (903191108)</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">CS 6476 Computer Vision - Class Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Tech, Fall 2019</span>
<hr>

The code for this project can be found in <a href="https://github.com/raghavrajmittal/PetSwap">this repository</a>.
<br><br>
Previous project deliverables: <a href="proposal.html">proposal</a>


 <!--It has been adapted from the following open source repositories:
<ul>
  <li><a href="https://github.com/matterport/Mask_RCNN">Mask R-CNN official implementation</a></li>
</ul>-->


<br><br>
<!-- Goal -->
<h2>Abstract</h2>
The purpose of this project is to match images of dogs with images of cats (and vice versa) based on color and texture. For example, given an input image of a cat, we wish to find an image of a dog from our dataset that it most similar in terms of fur/skin color and pattern. Our architecture consists of using Mask R-CNN to segment the animal from the image, obtaining a fused feature representation of color and texture features and cluster the images based on similarity.
The expected input is a real color image of a dog or a cat. The desired output is an image of the opposite animal type with the most similar fur/skin color and pattern.



<!-- <br><br> -->
<!-- figure -->
<!-- Main Illustrative Figure -->
<br><br>
<div style="text-align: center;">
<img style="height: 600px;" alt="" src="img/architecture.png">
<br>
Planned pipeline of Pet Swap
</div>

<br><br><br>
<div style="text-align: center;">
<img style="height: 200px;" alt="" src="img/teaser_figure.png">
<br>
Proposed output
</div>






<!-- <br><br> -->
<!-- Introduction -->
<h2>Introduction</h2>
We were inspired to do this project from this <a href="https://twistedsifter.com/2016/09/kittens-and-their-matching-bunnies/">article</a>. This is an application of <a href='https://en.wikipedia.org/wiki/Content-based_image_retrieval'>content-based image retrieval</a>. There has been a lot of research on CBIR, and we are performing this retrieval based on color and texture, which is also common practice. One thing that we have seen less of is the juxtaposition of image segmentation and CBIR. There are studies that address this, but we aim to see how well this combined approach performs. At the end of the day, we believe that this is just an interesting and fun application of various computer vision techniques.



<br><br><br>
<!-- Approach -->
<h2>Approach</h2>
<h4>Summary</h4>
Our system contains three distinct components: animal image segmentation, color/texture representation, and clustering. With these three steps we will be able to take any input image of a dog or a cat and output an image with the most similar color and texture. We can simply query our pre-trained clusters with the color/texture representation of the image and return the closest image.
<br><br>
<h4>Animal Image Segmentation</h4>
We use <a href='https://github.com/matterport/Mask_RCNN'>Mask R-CNN</a> to segment the animal out of the image. This model has already been trained on the COCO dataset, which contains color images and segmentation masks of dogs and cats (among many other objects). We used Mask R-CNN to segment animals out of our two datasets (see Datasets subsection under Experiments and Results). The next step is to feed the segmented animals into our texture/color histogram generator and map each feature representation to the image using a dictionary.
<br><br>
<h4>Color/Texture Representation</h4>
Once we produce the segmented animal image, we create histograms from the image using a variety of feature extraction techniques. We will experiment with these <a href='https://github.com/pochih/CBIR'>feature representations</a> to determine the best possible representation for animal images. The first two we will test are color-based and texture-based feature extraction. We suspect that a fusion of these two features will produce the best clustering result. We then feed the resulting color/texture histograms for each image as input to the next step - the clustering algorithm.
<br><br>
<h4>Clustering</h4>
We cluster images of dogs and cats separately by histogram similarity. We will start by running clustering to find similarities between histograms. Once we have clustered our training data from the dog and cat datasets, we will evaluate the performance by testing it with our test sets (see Datasets subsection). When there is a query to our system, we first find the closest cluster center. Once we are within the cluster, we select the closest histogram. Doing so will reduce the time complexity of the algorithm, as it removes the need to compare the new histogram with that of every image in the dataset to find the closest match. For dog input images, we query the cat cluster model and for cat input images, we query the dog cluster model. The chosen image is then returned to the user.


<br><br><br>
<!-- Results -->
<h2>Experiments and Results</h2>

<h4>Experimental Setup</h4>
We will divide the dataset into a 80% "training" and 20% testing split. In this case, the training data is the pre-clustered images. In order to test the functionality of our application, we will take the testing data and measure the sum of squared errors (SSE) of the clusters as well as the similarity between the input image and the returned output image.
<br><br>
<h4>Datasets</h4>
We plan to use existing datasets containing images of cats and dogs. We are using a pre-trained Mask R-CNN for image segmentation as mentioned above. This is trained on the <a href='http://cocodataset.org'>COCO dataset</a>.<br>We also have a dataset of cat images, the <a href="https://archive.org/details/CAT_DATASET">Cat Dataset</a>, which includes 10,000 cat images. Lastly, we have a dataset of dog images, the <a href="http://vision.stanford.edu/aditya86/ImageNetDogs/">Stanford Dogs Dataset</a>, which includes over 20,000 images.
<br>Additionally, if needed, we can scrape Google Images for more images if we find the datasets do not contain enough images.
<br><br>
<h4>Existing Code</h4>
We plan to use code from several GitHub repositories:
<div><a href="https://github.com/matterport/Mask_RCNN">Mask R-CNN</a>
<div><a href="https://github.com/pochih/CBIR">Content-based image retrieval (CBIR) system</a>
<div><a href="https://docs.opencv.org/3.4/d8/d83/tutorial_py_grabcut.html">GrabCut algorithm</a>
<br>Also, we plan to use an existing implementation of a clustering algorithm: either k-means, k-modes or expectation maximization (EM), which can be found online:
<div><a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">k-means</a>
<div href="https://github.com/nicodv/kmodes"><a>k-modes</a>
<div><a href="https://scikit-learn.org/stable/modules/mixture.html">EM</a>
<br><br>
<h4>Implementation Details</h4>
We are implementing the pipeline to solve the problem end-to-end. Specifically, we need to create a useful image representation that captures the key information about the images. This could be a combination of texture, color, etc.
<br><br>
<h4>Defining Success</h4>
Mathematically, we define success for the project to be the elbow of the k vs SSE curve and having a distance of 10% (of the mean distance) between input and output images. We would also evaluate how close the dogs and cats look by have multiple people using the service and reviewing it.
<br><br>
<h4>Experiments</h4>
<h5>Mask R-CNN</h5>
We will test how effective this pre-trained Mask R-CNN is. We hypothesize that it will be accurate when segmenting the dog and cat images from the other datasets. If we find that this model isn’t as accurate, we will try training it on some of the images from the other datasets as well to see if it improves accuracy significantly. <br>
<h5>Foreground Feature Representation</h5>
We can try examining different features of the foreground. As a baseline, we can add every image pixel into a histogram. However, there is a concern that this will be too computationally expensive. It could be more effective and simpler to consider specific features of the image such as texture or color. <br>
<h5>Distance Function</h5>
We can experiment with using different distance functions. To calculate similarity between histograms, two common functions are Euclidean distance and chi-squared. Unlike Euclidean which penalizes aboslute difference, chi-squared penalizes relative difference. Therefore, we expect the final clusters that result from using these two distance functions to be quite different. <br>
<h5>Clustering Algorithm</h5>
There are details of our clustering process that we need to determine through experiments. First, we need to decide whether we would cluster the dogs and cats separately or all together.<br>
Next, we would experiment with the clustering methods. We can try both hard and soft clustering. Hard clustering assigns each point to a cluster definitively, soft clustering uses a probabilistic approach to assign probabilities of belonging to each cluster for each point. Both could provide interesting results in this project since we expect hard clustering to give cleaner results faster but soft clustering to perhaps find some insights missing from hard clustering.<br>
Another detail would be what hard clustering algorithm we would utilize. We would decide between centroid-based and density-based, i.e. mean clustering vs. mode clustering. We would figure out these two details by comparing the accuracy of each implementation. <br>
Additionally, we can try varying the value of k in hard clustering. This is key to finding meaningful groups in the dataset. If the clusters are too small, similar images may not be grouped together. On the other, clusters that are too large will group dissimilar images together. We would decide this using the elbow method, trying to find the optimal value of k.


<h2>Qualitative Results</h2>
Using the Mask R-CNN implementation, and updating it to detect cats and dogs only, we can produce the following succesful segmentation.
<br><br>
<div style="text-align: center;">
  <img style="height: 200px;" alt="" src="img/mask_rcnn_output.png">
  <br>
  Output of Mask R-CNN
  </div>


<h2>Conclusion/Future Work</h2>
We have quite a few things to achieve for our final project update. The first thing we have to do run clustering on the feature representations of the images to ensure that our feature representations accurately represent the image content we are trying to retrieve. This can also help us in ensuring that the specifications of our clustering algorithm are good. Based on this, we can update our feature representation and clustering accordingly. We also need to implement an effective way of storing our data such that it can be accessed easily, and information about the closest cluster center and feature representation can be stored. Lastly, we need to create an interface for our project so it can be easily used.

<h2>References</h2>
<ol>
  <li><a href="https://github.com/matterport/Mask_RCNN">Mask R-CNN</a></li>
  <li><a href="https://www.researchgate.net/publication/220595166_Image_Clustering_using_Color_Texture_and_Shape_Features">CBIR with Color and Texture</a></li>
  <li><a href="https://link.springer.com/chapter/10.1007/978-3-642-27329-2_105">Using Gray Level Co-occurence Matrix</a></li>
</ol>


<br><br>
<!-- Footer -->
  <hr>
  <footer>
  <p>© Hemanth Chittanuru, Kenny Scharm, Raghav Raj Mittal, Sarah Li</p>
  </footer>
</div>
</div>

<br><br>

</body></html>
